# Model Configuration
[model]
name = "meta-llama/Llama-3.2-1B"        # Model identifier
enable_sae = true               # Master switch for SAE features

# Prompt Settings
[prompt]
example_prompt = "Once upon a time, there was a"
max_prompts = 9

# Display Settings
[display]
tokens_to_show = 30          # Number of tokens to display in preview
cache_size = 100            # Maximum number of cached token sequences

# SAE Settings
# Using transcoders for meta-llama/Llama-3.2-1B
# See: https://huggingface.co/mntss/skip-transcoder-Llama-3.2-1B-131k-nobos
# Note: Only layers 0-15 are available
[[sae]]
layer = 8
repo_id = "mntss/skip-transcoder-Llama-3.2-1B-131k-nobos"
filename = "layer_8.safetensors"
revision = "new-training"
